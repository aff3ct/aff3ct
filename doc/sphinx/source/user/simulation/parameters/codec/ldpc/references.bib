@Article{Peeling2001,
  author   = {M.G. Luby ; M. Mitzenmacher ; M.A. Shokrollahi ; D.A. Spielman},
  journal  = {IEEE Transactions on Information Theory (TIT)},
  title    = {Efficient erasure correcting codes},
  year     = {2001},
  volume   = {47},
  number   = {2},
  pages    = {569 - 584},
  abstract = {We introduce a simple erasure recovery algorithm for codes derived from cascades of sparse bipartite graphs and analyze the algorithm by analyzing a corresponding discrete-time random process. As a result, we obtain a simple criterion involving the fractions of nodes of different degrees on both sides of the graph which is necessary and sufficient for the decoding process to finish successfully with high probability. By carefully designing these graphs we can construct for any given rate R and any given real number /spl epsiv/ a family of linear codes of rate R which can be encoded in time proportional to ln(1//spl epsiv/) times their block length n. Furthermore, a codeword can be recovered with high probability from a portion of its entries of length (1+/spl epsiv/)Rn or more. The recovery algorithm also runs in time proportional to n ln(1//spl epsiv/). Our algorithms have been implemented and work well in practice; various implementation issues are discussed.},
  keywords = {error correction codes, linear codes, decoding, probability, graph theory, random processes, LDPC},
  doi      = {10.1109/18.910575},
  ISSN     = {0018-9448},
  month    = feb,
}

@InProceedings{Yeo2001,
  author    = {E. Yeo and P. Pakzad and B. Nikolic and V. Anantharam},
  title     = {High Throughput Low-Density Parity-Check Decoder Architectures},
  booktitle = {Global Communications Conference (GLOBECOM)},
  year      = {2001},
  volume    = {5},
  pages     = {3019--3024 vol.5},
  publisher = {IEEE},
  abstract  = {Two decoding schedules and the corresponding serialized architectures for low-density parity-check (LDPC) decoders are presented. They are applied to codes with parity-check matrices generated either randomly or using geometric properties of elements in Galois fields. Both decoding schedules have low computational requirements. The original concurrent decoding schedule has a large storage requirement that is dependent on the total number of edges in the underlying bipartite graph, while a new, staggered decoding schedule which uses an approximation of the belief propagation, has a reduced memory requirement that is dependent only on the number of bits in the block. The performance of these decoding schedules is evaluated through simulations on a magnetic recording channel},
  comment   = {Article qui introduit l'horizontal layered pour la première fois.},
  doi       = {10.1109/GLOCOM.2001.965981},
  file      = {:pdf/Yeo2001 - High Throughput Low-Density Parity-Check Decoder Architectures.pdf:PDF},
  groups    = {LDPC Codes},
  keywords  = {Galois fields, belief maintenance, decoding, error detection codes, magnetic recording, Galois fields, LDPC decoders, belief propagation, bipartite graph, concurrent decoding, decoding schedules, low-density parity-check decoders, magnetic recording channel, parity check matrices, serialized architectures, staggered decoding schedule, Bipartite graph, Computational modeling, Computer architecture, Galois fields, Iterative decoding, Magnetic recording, Message passing, Parity check codes, Processor scheduling, Throughput, layered, horizontal layered},
}

@InProceedings{Zhang2002,
  author    = {J. Zhang and M. Fossorier},
  title     = {Shuffled Belief Propagation Decoding},
  booktitle = {Asilomar Conference on Signals, Systems, and Computers (ACSSC)},
  year      = {2002},
  volume    = {1},
  pages     = {8--15 vol.1},
  month     = nov,
  publisher = {IEEE},
  abstract  = {In this paper, we propose a shuffled version of the belief propagation (BP) algorithm for the decoding of low-density parity-check (LDPC) codes. We show that when the Tanner graph of the code is acyclic and connected, the proposed scheme is optimal in the sense of MAP decoding and converges faster (or at least no slower) than the standard BP algorithm. Interestingly, this new version keeps the computational advantages of the forward-backward implementations of BP decoding. Both serial and parallel implementations are considered. We show by simulation that the new schedule offers better performance/complexity trade-offs.},
  comment   = {C'est le papier qui a introduit le vertical layered.},
  doi       = {10.1109/ACSSC.2002.1197141},
  file      = {:pdf/Zhang2002 - Shuffled Belief Propagation Decoding.pdf:PDF;:pdf/Zhang2002 - Shuffled Belief Propagation Decoding [slides].pdf:PDF},
  groups    = {LDPC Codes},
  issn      = {1058-6393},
  keywords  = {computational complexity, graph theory, maximum likelihood decoding, maximum likelihood estimation, parity check codes, BP algorithm, LDPC codes, MAP decoding, Tanner graph, computational complexity, forward-backward implementations, low-density parity-check codes, maximum a posteriori probability, parallel implementation, serial implementation, shuffled belief propagation decoding, Belief propagation, Code standards, Computational modeling, Concurrent computing, Iterative algorithms, Iterative decoding, Parallel processing, Parity check codes, Processor scheduling, Standards development, vertical layered, layered scheduling, layered},
}

@InProceedings{MacKay1995,
  author    = {D. J. C. MacKay and R. M. Neal},
  title     = {Good Codes Based on Very Sparse Matrices},
  booktitle = {IMA International Conference on Cryptography and Coding (IMA-CCC)},
  year      = {1995},
  pages     = {100--111},
  address   = {UK},
  month     = dec,
  publisher = {Springer},
  doi       = {10.1007/3-540-60693-9_13},
  file      = {:pdf/MacKay1995 - Good Codes Based on Very Sparse Matrices.pdf:PDF},
  groups    = {LDPC Codes},
  isbn      = {978-3-540-49280-1},
}

@Article{Wadayama2010,
  author   = {T. Wadayama and K. Nakamura and M. Yagita and Y. Funahashi and S. Usami and I. Takumi},
  journal  = {IEEE Transactions on Communications (TCOM)},
  title    = {Gradient Descent Bit Flipping Algorithms for Decoding LDPC Codes},
  year     = {2010},
  volume   = {58},
  number   = {6},
  pages    = {1610-1614},
  abstract = {A novel class of bit-flipping (BF) algorithm for decoding low-density parity-check (LDPC) codes is presented. The proposed algorithms, which are referred to as gradient descent bit flipping (GDBF) algorithms, can be regarded as simplified gradient descent algorithms. The proposed algorithms exhibit better decoding performance than known BF algorithms, such as the weighted BF algorithm or the modified weighted BF algorithm for several LDPC codes.},
  keywords = {decoding;gradient methods;parity check codes;gradient descent bit flipping algorithms;decoding LDPC codes;low-density parity-check;weighted BF algorithm;Parity check codes;Iterative decoding;Bit error rate;Vectors;Sum product algorithm;Algorithm design and analysis;Belief propagation;Linear code;Galois fields;Communications Society;code, bit-flipping algorithm, gradient descent algorithm.},
  doi      = {10.1109/TCOMM.2010.06.090046},
  ISSN     = {0090-6778},
  month    = jun,
}

@Book{Ryan2009,
  title     = {Channel codes: classical and modern},
  publisher = {Cambridge University Press},
  year      = {2009},
  author    = {W. Ryan and S. Lin},
  month     = sep,
  isbn      = {978-0-511-64182-4},
  abstract  = {Channel coding lies at the heart of digital communication and data storage, and this detailed introduction describes the core theory as well as decoding algorithms, implementation details, and performance analyses.
Professors Ryan and Lin, known for the clarity of their writing, provide the latest information on modern channel codes, including turbo and low-density parity-check (LDPC) codes. They also present detailed coverage of BCH codes, Reed–Solomon codes, convolutional codes, finite-geometry codes, and product codes, providing a one-stop resource for both classical and modern coding techniques.
The opening chapters begin with basic theory to introduce newcomers to the subject, assuming no prior knowledge in the field of channel coding. Subsequent chapters cover the encoding and decoding of the most widely used codes and extend to advanced topics such as code ensemble performance analyses and algebraic code design. Numerous varied and stimulating end-of-chapter problems, 250 in total,
are also included to test and enhance learning, making this an essential resource for students and practitioners alike.},
  file      = {:pdf/Ryan2009 - Channel codes\: classical and modern.pdf:PDF},
  groups    = {Error-Correcting Codes (ECC)},
  url       = {http://www.cambridge.org/9780521848688},
}

@Article{Fossorier1999,
  author   = {M. P. C. Fossorier and M. Mihaljevic and H. Imai},
  journal  = {IEEE Transactions on Communications (TCOM)},
  title    = {Reduced Complexity Iterative Decoding of Low-Density Parity Check Codes based on Belief Propagation},
  year     = {1999},
  volume   = {47},
  number   = {5},
  pages    = {673-680},
  abstract = {Two simplified versions of the belief propagation algorithm for fast iterative decoding of low-density parity check codes on the additive white Gaussian noise channel are proposed. Both versions are implemented with real additions only, which greatly simplifies the decoding complexity of belief propagation in which products of probabilities have to be computed. Also, these two algorithms do not require any knowledge about the channel characteristics. Both algorithms yield a good performance-complexity trade-off and can be efficiently implemented in software as well as in hardware, with possibly quantized received values.},
  keywords = {error detection codes;iterative decoding;computational complexity;probability;belief maintenance;AWGN channels;binary codes;block codes;low-density parity check codes;reduced complexity iterative decoding;belief propagation algorithm;fast iterative decoding;additive white Gaussian noise channel;decoding complexity;probabilities product;performance-complexity trade-off;software;hardware;quantized received values;binary code;block code;Iterative decoding;Parity check codes;Belief propagation;Iterative algorithms;Approximation algorithms;AWGN;Code standards;Bit error rate;Additive white noise;Standards development},
  doi      = {10.1109/26.768759},
  ISSN     = {0090-6778},
  month    = may,
}

@Article{Chen2002,
  author   = {J. Chen and M. P. C. Fossorier},
  journal  = {IEEE Communications Letters (COMML)},
  title    = {Density Evolution for Two Improved BP-Based Decoding Algorithms of LDPC Codes},
  year     = {2002},
  volume   = {6},
  number   = {5},
  pages    = {208-210},
  abstract = {In this letter, we analyze the performance of two improved belief propagation (BP) based decoding algorithms for LDPC codes, namely the normalized BP-based and the offset BP-based algorithms, by means of density evolution. The numerical calculations show that with one properly chosen parameter for each of these two improved BP-based algorithms, performances very close to that of the BP algorithm can be achieved. Simulation results for LDPC codes with code length moderately long validate the proposed optimization.},
  keywords = {iterative decoding;block codes;belief maintenance;AWGN channels;belief propagation based decoding algorithms;LDPC codes;normalized BP-based algorithms;offset BP-based algorithms;density evolution;numerical calculations;iterative decoding;low-density parity-check codes;Parity check codes;Iterative algorithms;Iterative decoding;Random variables;Performance analysis;Algorithm design and analysis;Belief propagation;Sum product algorithm;Degradation;Convergence},
  doi      = {10.1109/4234.1001666},
  ISSN     = {1089-7798},
  month    = may,
}

@Book{Declerq2014,
  title     = {Channel Coding: Theory, Algorithms, and Applications},
  publisher = {Academic Press},
  year      = {2014},
  author    = {D. Declerq, M. Fossorier and E. Biglieri},
  isbn      = {978-0-12-396499-1},
  abstract  = {This book gives a review of the principles, methods and techniques of important and emerging research topics and technologies in Channel Coding, including theory, algorithms, and applications. Edited by leading people in the field who, through their reputation, have been able to commission experts to write on a particular topic. With this reference source you will: Quickly grasp a new area of research. Understand the underlying principles of a topic and its applications. Ascertain how a topic relates to other areas and learn of the research issues yet to be resolved},
  doi       = {10.1016/C2011-0-07211-3},
}
